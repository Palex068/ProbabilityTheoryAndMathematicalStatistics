# Регрессионный анализ

Цель лекции:
+	Изучить линейную регрессию на примере парной линейной регрессии
+	Познакомиться с различными методами построения модели
+	Построить модель линейной регрессии
+	Научиться оценивать полученную математическую модель с помощью различных метрик
+	Познакомиться с логистической регрессией.

Материал прошлого урока:

Мы уже обсудили с вами, что такие методы как корреляционный анализ и регрессионный анализ помогают проследить нам зависимость одной переменной от другой. 

Но если корреляционный анализ помогает нам оценить тесноту связи, то регрессионный анализ может ответить на вопрос, как именно изменяется зависимая переменная y при изменении независимой переменной $x$.

## План лекции 9:
1. [Регрессионный анализ](#регрессионный-анализ)
2. [Парная линейная регрессия](#парная-линейная-регрессия)
3. [Метод наименьших квадратов](#метод-наименьших-квадратов)
4. [Условия применимости линейной регрессии](#условия-применимости-линейной-регрессии)
5. [Трансформация данных](#трансформация-данных)
6. [Поиск коэффициентов для парной линейной регрессии](#поиск-коэффициентов-для-парной-линейной-регрессии)
7. [Функция потерь](#функция-потерь)
8. [Матричный метод расчета коэффициентов линейной регрессии](#матричный-метод-расчета-коэффициентов-линейной-регрессии)
9. [Расчет коэффициентов методом градиентного спуска](#расчет-коэффициентов-методом-градиентного-спуска)
10. [Функция в Python для построения линейной регрессии](#функция-в-python-для-построения-линейной-регрессии)
11.	[Коэффициент детерминации](#коэффициент-детерминации)
12. [Оценка значимости математической модели](#оценка-значимости-математической-модели)
13. [Оценка значимости отдельных коэффициентов](#оценка-значимости-отдельных-коэффициентов)
14. [Логистическая регрессия](#логистическая-регрессия)
15. [Решение задач](#решение-задач-9)
16. [Домашняя работа](/HW009.ipynb)

[Содержание курса](#план-курса)

## Регрессионный анализ

Мы уже обсудили с вами, что такие методы как корреляционный анализ и регрессионный анализ помогают проследить нам зависимость одной переменной от другой. 

Но если корреляционный анализ помогает нам оценить тесноту связи, то регрессионный анализ может ответить на вопрос, как именно изменяется зависимая переменная y при изменении независимой переменной $x$. 

Регрессионный анализ помогает нам подобрать математическую модель определенного типа, с помощью которой мы опишем изменение зависимой переменной у при изменении независимой переменной $x$.

Давайте вернемся к тем двум переменным, площадь и цена, с которыми мы работали на прошлом уроке. 

У нас есть $8$ парных значений, мы можем нанести их на график синими точками. 

Каждая точка имеет $2$ координаты (площадь, цена). <br>
Площадь здесь будет независимой переменной $x$(признак, причина), а цена квартиры – зависимой переменной $y$ (результирующая, следствие). 

![Регрессионный анализ](/Pictures/009_001.png)

Но мы могли бы иметь несколько признаков, например, удаленность от метро. <br>
Тогда бы каждая точка на графике задавалась бы $3$ координатами. 

И такие данные уже будут называться многомерными. <br>
Занимается изучением этих данных многомерный статистический анализ.

Т.е. __многомерный статистический анализ__ – это раздел статистики, который посвящен исследованиям экспериментов с многомерными наблюдениями.  

Вернемся к этому понятию чуть позже.  <br>
Обратите внимание, что регрессии могут быть, как линейные, так и нелинейные. <br>
И начнем с самого простого и очень часто используемого метода парной линейной регрессии.

Но давайте еще раз проговорим, для чего нужна математическая модель?  

С помощью нее мы сможем предсказывать результирующую переменную. <br>
Мы подберем модель, а затем, имея только значение признака, сможем предсказать результирующую. 

В нашем случае с помощью площади сможем предсказать стоимость квартиры.

![Регрессионный анализ](/Pictures/009_037.PNG)

![Регрессионный анализ](/Pictures/009_038.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Парная линейная регрессия

![Регрессионный анализ](/Pictures/009_039.PNG)

Если признак один, то такая линейная регрессия называется парной. <br>
Она описывает связь признака $x$ с результирующей $y$. 

В основе математической модели, описывающей эту зависимость, лежит предположение, что зависимость выражена следующим образом:

$y = a + bx$

Это уравнение прямой. <br>
Чтобы сделать такое предположение, нам нужно для начала взглянуть на график и оценить, прослеживается ли между переменными линейная зависимость. <br>
Эти данные знакомы нам уже с прошлого урока, поэтому мы точно знаем, что линейная зависимость есть, к тому же, посмотрев на расположение синих  точек на графике, можно убедиться, что она напоминает прямую.

Давайте для начала рассмотрим коэффициенты $a$ и $b$ в данном уравнении. <br>
Коэффициент $a$ называется в статистике __интерсептом__ (перехватом).

Посмотрим на два рисунка ниже. <br>
На них представлены $2$ математические модели в виде уравнения и графика. <br>
Если мы мысленно продолжим фиолетовую прямую, то она пересечет ось $у$, когда его значение будет  равно $3$. <br>
Т.е.  зависимая переменная равна $3$, когда независимая переменная равна $0$. 

Значение $3$ и будет являться __интерсептом__. <br>
Для обеих моделей он одинаков. 

![Регрессионный анализ](/Pictures/009_002.png)

Также мы видим, что на рисунке $А$ зависимая переменная изменяется не так быстро, как на рисунке $Б$. <br>
Давайте построим простые таблицы, как мы делали в школе, для обоих графиков. 

Независимую переменную будем изменять на $1$.

![Регрессионный анализ](/Pictures/009_003.png)

В таблице для математической модели $А$ мы видим, что $x$ изменяется на $1$, а при этом y  изменяется на $2$ единицы. <br>
А в таблице, построенной для математической модели $Б$, видно, что при изменении $x$ на единицу  y при этом меняется на $3$ единицы.  

Т.е. коэффициент $b$ перед признаком $x$  показывает, на сколько единиц изменяется зависимая переменная y при изменении $x$ на $1$. <br>
Вот мы и видим, что модель $Б$ описывает данные, где $y$ изменяется быстрее, чем в данных, описанных с помощью  модели $А$.

![Регрессионный анализ](/Pictures/009_040.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Метод наименьших квадратов

![Регрессионный анализ](/Pictures/009_041.PNG)

Чтобы построить линейную регрессию для начала нам надо накопить данные. <br>
Эти данные мы будем использовать, чтобы подобрать коэффициенты.  

У нас есть все те же данные, площадь и цена, мы убедились на прошлом уроке, что они имеют линейную зависимость, теперь запишем модель, которая будет описывать эту зависимость.

$\hat{y}=β_0+β_1 \cdot x$

В статистике  вместо коэффициентов $a$ и $b$ принято использовать коэффициенты $β$.

Имея данные, мы подбираем коэффициенты $β$ таким образом, что прямая (графическая математическая модель) проходит максимально близко ко всем точкам. <br>
Т.е. коэффициенты подбираются так, что сумма квадратов расстояний от синих точек до прямой минимальна. 

Иными словами, в основе линейной регрессии лежит метода наименьших квадратов. <br>
После того, как мы подберем коэффициенты, мы можем подставить признак $(x)$ в это уравнение и посчитать оценочные значения $\hat{y}$ ($y$  с крышкой).

![Регрессионный анализ](/Pictures/009_004.png)

![Регрессионный анализ](/Pictures/009_049.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Условия применимости линейной регрессии

Помимо предварительной оценки на наличие линейной зависимости, есть еще три условия применимости. Перечислим все условия применимости ниже:
1. Наличие линейной зависимости между зависимой и независимой переменной.
2. Независимость остатков
3. Для любого значения x значение зависимой переменной y распределено нормально.
4. Гомоскедастичность.

Теперь каждое условие рассмотрим более подробно. 

1.	Наличие линейной зависимости можно увидеть с помощью графика. <br>
На первом графике условие соблюдается, на втором, где видим параболу, нет.

![Регрессионный анализ](/Pictures/009_005.png)

Решить эту проблему можем с помощью нелинейной трансформации $x$ и/или $y$. <br>
Здесь мы можем применить:
+	логарифм, 
+	квадратный корень 
+	умножение на обратное число

![Регрессионный анализ](/Pictures/009_010.PNG)

2. Независимость остатков

Проверить можем с помощью графика, на котором с течением времени мы не должны видеть каких-то паттернов в поведении остатков. <br>
Например, с течением времени остатки не должны расти. <br>
Они должны быть разбросаны случайным образом вокруг горизонтальной линии, как, например, на рисунке ниже.

![Регрессионный анализ](/Pictures/009_006.png)

А если мы видим расположение остатков, например, напоминающих параболу, то мы приходим к выводу, что для таких данных лучше подойдет нелинейная модель.

![Регрессионный анализ](/Pictures/009_007.png)

Эта проблема тоже решаема с помощью трансформации данных.

![Регрессионный анализ](/Pictures/009_011.PNG)

3. Нормальное распределение остатков. Проверить данное утверждение можно после построение математической модели линейной регрессии.

Методы проверки:
+	Тест Шапиро-Уилка
+	QQ –график

![Регрессионный анализ](/Pictures/009_008.png)

Мы видим, что статистика $0.96$, а $p-value = 0.81$. 

Если сравнить с заранее установленным уровнем статистической значимости $α$, возьмем, как обычно, $0.05$, то видим что $p-value > α$. 

Если $p-value > α$, то делаем вывод всегда в пользу нулевой гипотезы. 

А нулевая гипотеза гласит о том, что различий нет. <br>
Т.е. мы не нашли отличия от нормального распределения.

А теперь убедимся в правильности вывода с помощью графика QQ-plot.

![Регрессионный анализ](/Pictures/009_009.png)

И здесь практически все точки лежат на прямой.

![Регрессионный анализ](/Pictures/009_012.PNG)

![Регрессионный анализ](/Pictures/009_050.PNG)

![Регрессионный анализ](/Pictures/009_051.PNG)

![Регрессионный анализ](/Pictures/009_052.PNG)

![Регрессионный анализ](/Pictures/009_053.PNG)

4. Гомоскедастичность. 

Под этим свойством понимается постоянство дисперсии при всех значениях $x$.

Например, если расположение остатков напоминает конус, мы говорим, о __гетероскедастичности__. Ниже на рисунке изображено два примера гетероскедастичности. На рисунке А с увеличением x растет дисперсия. На рисунке Б дисперсия максимальна при средних значениях x.

![Регрессионный анализ](/Pictures/009_014.png)

![Регрессионный анализ](/Pictures/009_013.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Трансформация данных

Чтобы обеспечить условия применимости линейной регрессии иногда нам необходимо сделать трансформацию данных. 

Но нельзя забывать, что получив модель, нам нужно сделать обратную трансформацию. <br>
Основные методы приведены в таблице ниже.

![Регрессионный анализ](/Pictures/009_015.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Поиск коэффициентов для парной линейной регрессии

3 метода построения линейной регрессии
+ Математические формулы
+ Матричный метод
+ Метод градиентного спуска


![Регрессионный анализ](/Pictures/009_016.PNG)

![Регрессионный анализ](/Pictures/009_017.PNG)

Если у нас есть модель вида  

$\hat{y}=b_0 + b_1 \cdot x$

то расчет коэффициентов производится по следующим формулам:

$b_1= \dfrac{n \cdot \sum_{i=1}^n x_i \cdot y_i - (\sum_{i=1}^n x_i)(\sum_{i=1}^n y_i)}{n \cdot \sum_{i=1}^n x_i^2-(\sum_{i=1}^n x_i)^2}$

$b_0= \bar y - b \cdot \bar x$

$\bar x = \dfrac{\sum_{i=1}^n x_i}{n}$

$\bar y = \dfrac{\sum_{i=1}^n y_i}{n}$

Сделаем эти вычисление ниже в Python. 

Один из расчетов ниже для коэффициента $b_1$  будет сделан по формуле, указанной выше. 

Другая формула ($2$ способ) также встречается в литературе и приводит к точно такому же результату. 

После того, как мы рассчитаем коэффициенты, получим модель вида:

$\hat {y} = 0.1715 + 0.0387 \cdot x$

Если мы обратно подставим в эту модель площадь и посчитаем соответствующую цену за квартиру, то это уже будут оценочные значения $y_{pred}$.

![Регрессионный анализ](/Pictures/009_018.png)

[Содержание лекции](#план-лекции-9)

<hr>

## Функция потерь

Как было сказано ранее, в основе метода линейной регрессии лежит метод наименьших квадратов. 

Т.е. коэффициенты подбираются таким образом, чтобы минимизировать среднюю квадратичную ошибку, которая находится по формуле:

$mse = \dfrac {\sum(y - y_{pred})^2}{n}$

$n$ – число измерений.

__Функция потерь $mse$ - мера измерения ошибок, которые функция делает на нашем наборе данных.__

![Регрессионный анализ](/Pictures/009_019.png)

![Регрессионный анализ](/Pictures/009_020.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Матричный метод расчета коэффициентов линейной регрессии

![Регрессионный анализ](/Pictures/009_054.PNG)

Чтобы решить матричным методом уравнение вида

$\hat {y} = b_0 + b_1 \cdot x$

нам нужно представить это уравнение в матричном виде. 

Матрица $X$ равна произведению матрицы $Y$ и матрицы $B$.

$Y = X \cdot B$

$(y_1 \space y_2 \space y_3  )= (1 x_1 \space 1 x_2 \space 1 x_3)  (β_0 \space β_1)$

Чтобы создать матрицы, необходимо массив площади и цены превратить в вектор-столбец с помощью атрибута _reshape_ (смотрите ниже). 

Когда у нас модель с интерсептом, то необходимо добавить в матрицу $X$ столбец единиц. 

В Python создаем столбец единиц через функцию _np.ones()_ и добавляем к вектору-столбцу $X$ с помощью функции _np.hstack()_.

После этого останется решить матричное уравнение. <br>
Не будем на этом курсе заниматься выводом формулы для расчета матрицы $B$. <br>
Вы можете воспользоваться уже выведенной формулой для дальнейшего применения в своих задачах. 

$\hat{B} =(X^T \cdot X)^{-1} \cdot X^T \cdot Y$

Перемножение матриц в Python делается через функцию _dot_  или _@_.

Решив матричное уравнение, получаем матрицу $B$. 

Матричный метод дал нам те же значения коэффициентов, что и метод с использованием формул.

Матричный метод мы также можем использовать и при многомерном анализе. <br>
Тогда у нас будет больше столбцов (признаков) в матрице $X$  и, соответственно, матрица B даст больше коэффициентов.

Если по какой-то причине мы хотим построить модель без интерсепта, тогда просто не добавляем столбец единиц.

![Регрессионный анализ](/Pictures/009_021.png)

```python
matrix_B = (matrix_X.T @ matrix_X).I @ matrix_X.T @ matrix_Y
```

[Содержание лекции](#план-лекции-9)

<hr>

## Расчет коэффициентов методом градиентного спуска

Градиентный спуск – итеративный метод машинного обучения. 

Мы рассмотрим этот алгоритм поиска коэффициентов для модели вида без интерсепта.

$\hat{y}=β_1 \cdot x$

Для начала напишем в Python  функцию $mse$, чтобы потом сравнить расчеты $mse$ через функцию с вычислениями $mse$ с помощью метода градиентного спуска. 

Коэффициенты подбираются так, чтобы минимизировать эту ошибку. <br>
В этом методе мы работаем с производной $mse$, причем производную берем по коэффициенту, который ищем, а все остальные переменные рассматриваем как константы.

Нам понадобится установить скорость обучения $alpha$. 

Нет четких правил по выбору этого параметра. <br>
Если мы возьмем слишком большой, то не сможем посчитать коэффициент, проскакивая через минимум, а если слишком маленький, то вычисления могут занять очень много времени.

Также надо определиться со стартовыми значениями коэффициентов. 

У нас модель без интерсепта, поэтому только определяем стартовое значение для $β_1 = 0.1$. (Обычно значение берут из нормального стандартного распределения). 

В примере ниже взяли $10$ итераций и скорость обучения $alpha = 0.000001$. 

Коэффициент на $10$-й  итерации продолжает изменяться.

![Регрессионный анализ](/Pictures/009_022.png)

Теперь увеличим число итераций  до $3000$ и будем выводить каждую $500$ итерацию и помимо коэффициента будем выводить и $mse$.

![Регрессионный анализ](/Pictures/009_023.png)

Мы видим, что в последних $2$-х выведенных итерациях  mse  уже не меняется до $14$-го знака после запятой и  изменение коэффициента также остановилось. <br>
Мы получили модель вида:
$\hat{y}= 0.0417 \cdot x$

[Содержание лекции](#план-лекции-9)

<hr>

## Функция в Python для построения линейной регрессии

В Python также есть метод для построения линейной регрессии. 

Сначала мы задаем математическую модель. <br>
Затем превращаем независимую переменную $s$ в двумерный массив, и после этого  подбираем коэффициенты. <br>
Комментарии также оставлены в коде.

![Регрессионный анализ](/Pictures/009_024.png)

Построим график, где линия –  модель линейной регрессии, а точки – истинные значения.

![Регрессионный анализ](/Pictures/009_025.png)

![Регрессионный анализ](/Pictures/009_026.png)

После того как построили модель линейной регрессии мы должны оценить полученную модель. 

Нам понадобятся такие параметры как:
+ коэффициент детерминации,
+ критерий Фишера и
+ критерий Стьюдента.

[Содержание лекции](#план-лекции-9)

<hr>

## Коэффициент детерминации

![Регрессионный анализ](/Pictures/009_042.PNG)

Коэффициент детерминации показывает, какую долю изменчивости $y$ описала подобранная математическая модель. 

Коэффициент детерминации равен квадрату коэффициента корреляции и обозначается
$R^2= r^2$.

![Регрессионный анализ](/Pictures/009_027.png)

Это означает, что приблизительно $95.8\%$ изменчивости описано с помощью подобранной математической модели, что является очень хорошим показателем.

![Регрессионный анализ](/Pictures/009_043.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Оценка значимости математической модели

Для оценки значимости математической модели используется критерий Фишера. 

![Регрессионный анализ](/Pictures/009_044.PNG)

Критерий Фишера имеет ассиметричное распределение с правым длинным хвостом. <br>
Как и в любом тестировании гипотезы, нам надо сравнить табличное значение коэффициента с расчетным. 

Табличное значение выбираем по таблице значений $F$ - критерия (таблица Фишера- Снедекора) для заданного уровня статистической значимости. <br>
Установим заранее уровень статистической значимости $α$, как обычно, $0.05$.

Рассчитаем критерий  Фишера F в  Python 

![Регрессионный анализ](/Pictures/009_028.png)

Сравним расчетное значение с табличным значением, которое можем получить либо с помощью функции _f.ppf_, либо с помощью таблицы значений критерия Фишера.

![Регрессионный анализ](/Pictures/009_029.png)

Осталось сделать вывод. 

Табличное значение разбивает график на область принятия нулевой гипотезы  и альтернативной. 

Критическое значение $5.99$, а расчетное $135.55$, т.е. попадает в область принятия гипотезы $H_1$. 

Т.е. наша модель признана статистически значимой на уровне $α= 0.05$

![Регрессионный анализ](/Pictures/009_030.png)

![Регрессионный анализ](/Pictures/009_045.PNG)

[Содержание лекции](#план-лекции-9)

<hr>

## Оценка значимости отдельных коэффициентов

![Регрессионный анализ](/Pictures/009_046.PNG)

![Регрессионный анализ](/Pictures/009_047.PNG)

![Регрессионный анализ](/Pictures/009_048.PNG)

Помимо оценки статистической значимости модели в целом, также используют оценку значимости отдельных коэффициентов. 

Для этого воспользуемся критерием Стьюдента. 

Критерий Стьюдента рассчитываем:  значение коэффициента разделить на его стандартную ошибку. 

Вычисления приведены ниже. 

$Mso$ находили выше при расчете критерия Фишера.

![Регрессионный анализ](/Pictures/009_031.png)

Найдем табличный критерий Стьюдента для $α = 0.05$ и степеней свободы $n - 2 = 6$.

Помним, что здесь тест двусторонний, поэтому в функции указываем значение $(1 -   α/2)$. 

Табличный критерий $2.447$.

![Регрессионный анализ](/Pictures/009_032.png)

Критерий Стьюдента для коэффициента $β_0$,равный $0.924$, попадает в область принятия нулевой гипотезы, т.е.  коэффициент не является статистически значимым, а вот $t_β  = 11.64$ для коэффициента $β_1$ является статистически значимым.

Давайте сравним две графические модели с интерсептом и без. <br>
Несмотря на то, что интерсепт в данной модели не является статистически значимым, мы его оставляем. 

И вообще на практике, всегда рекомендовано строить модели с интерсептом. 

Если мы взглянем  на $mse$ для модели без интерсепта $(0.02285)$ и на $mse$  для модели с интерсептом $(0.020)$, то видим, что все-таки для модели с интерсептом $mse$ меньше.

![Регрессионный анализ](/Pictures/009_033.png)

Итак, окончательная модель имеет вид

$\hat{y} = 0.1715 + 0.0387 \cdot x$

Модель является статистически значимой на уровне $α = 0.05$

Помимо линейной регрессии, встречаются и нелинейные регрессии. 

Одним из примеров таких моделей является логистическая регрессия. 

[Содержание лекции](#план-лекции-9)

<hr>

## Логистическая регрессия

Логистическая регрессия применяется, когда y является бинарной переменной $(0$ или $1)$. 

Т.е. с помощью этого метода мы можем решить задачу бинарной классификации.

Рассмотрим применения логистической регрессии в банковском деле, когда принимается решение о выдаче кредита. 

У нас есть уже некоторые данные, где мы знали о клиентах:
+ $zp$ - их заработную плату,
+ $prod$ - среднюю сумму, которую они оставляют на продукты,
+ $poezdky$ количество поездок за рубеж  и
+ $y$ результирующую (вернул - $1$ или не вернул кредит- $0$).

![Регрессионный анализ](/Pictures/009_034.png)

Построим предварительную линейную модель _modl_.

![Регрессионный анализ](/Pictures/009_035.png)

Если мы будем подставлять данные по новым клиентам, то эта модель не обеспечит нас значениями в пределах от $0$ до $1$, поэтому мы должны применить трансформацию данных для $y$, полученного с помощью _model_. 

Применим сигмоидную функцию.

$sigmoid=  \dfrac{1}{1+e^{-modl}}$

![Регрессионный анализ](/Pictures/009_036.png)

Теперь $y$ – это вероятность, что клиент вернет кредит. 

Банк выбирает отсечку, выше которой y будет превращаться в $1$, т.е. такому клиенту кредит дадут. 

Сегодня мы познакомились с регрессионным анализом. 

На следующем уроке мы продолжим знакомство с критерием Фишера. 

Научимся его применять в регрессионном анализе, который позволяет делать множественные сравнения.

Резюме
1. Непосредственно факт наличия линейной взаимосвязи проверяется с помощью корреляционного анализа
2. Если линейная зависимость наблюдается, можно построить модель линейной регрессии. <br>
Она укажет на характер этой зависимости (т.е. на то, каким именно образом изменяется переменная под влиянием факторов)
3. С помощью F-критерия Фишера можно проверить, является ли уровень зависимости в данных статистически значимым
4. С помощью доверительных интервалов можно оценить реальный вклад каждого фактора в изменение переменно



[Содержание лекции](#план-лекции-9)

<hr>

## Решение задач 9

[Решение задач](/S009.ipynb)

### Задача $1$
Постройте графики для приведенных наборов данных. 

Найдите коэффициенты для линии регрессии и коэффициенты детерминации. 

Что вы замечаете? 

Нанесите на график модель линейной регрессии.

```python
X1= np.array([30,30,40, 40])
Y1= np.array([37, 47, 50, 60]) 
X2= np.array([30,30,40, 40, 20, 20, 50, 50]) 
y2= np.array([37, 47, 50, 60, 25, 35, 62, 72]) 
X3 = np.array([30,30,40, 40, 20, 20, 50, 50, 10, 10, 60, 60]) 
Y3 = np.array([37, 47, 50, 60, 25, 35, 62, 72, 13, 23, 74, 84]) 
```

### Задача $2$

На 8 уроке мы строили графики приведенных ниже данных.
Для какого графика можно использовать модель линейной регрессии?
```python
x = np.array([10,8, 13, 9,11,14, 6,4,12, 7,5])
y = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68 ])
x1= np.array([ 10,8, 13, 9,11,14, 6,4,12, 7,5 ])
y1 = np.array([ 9.14, 8.14, 8.74,8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74])
x2= np.array([ 10,8, 13, 9,11,14, 6,4,12, 7,5 ])
y2 = np.array([7.46,6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73])
x3 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])
y3 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25,12.5, 5.56, 7.91, 6.89])
x0= np.array([ 10, 8, 13, 9, 11, 14, 6, 4, 12, 7,5, 15, 16, 18 ])
y0 = np.array([ 9.14, 8.14, 8.74,8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74, 6.5, 5, 2.9])
```

[Содержание лекции](#план-лекции-9)

<hr>


[Содержание курса](/README.MD)

<hr>